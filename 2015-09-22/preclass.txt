0.  How much time did you spend on this pre-class exercise, and when?
    15 minutes-ish both before and after class :) Although I spend quite a bit of time on understanding
    pthreads, as I don't have much experience in that

1.  What are one or two points that you found least clear in the
    9/22 slide decks (including the narration)?
    I thought the MC simulation stuff could use a bit more clarification; for those without a probability theory background, 
    they'd find the error and convergence rate stuff a bit overwhelming. I personally like the
     "calculating pi with by finding the area of a circle via MC" (and I think the error analysis could 
     be sort of skipped over)
     
     Otherwise, it was pretty good. I also think that there could more of a intro to the whole
     "shared vs distributed memory" (which I felt was glossed over a bit in slide 3, right after the overview, 
     of the basics section) before actually delving into the nitty-gritty details, as this would 
     help give a lot of people a bit more perspective on what possible alternatives to 
     the shared memory model is
    

2.  The pthread_mc.c file in the demo subdirectory runs a Monte Carlo
    simulation to estimate the expected value of a uniform random
    variable.  The "-p" option sets the number of processors used,
    while "-b" sets the number of trials between synchronizations.

    a) Write a model for the run time for this simulation code in
       terms of the number of trials (N), number of processors (p),
       time per trial (t_trial), and time to update the global
       counters in the critical section (t_update).
    
    We will have the time per trial times number of trials plus time spend in the critical section for each processor, which     will be 
    N*t_trial + p*t_critical
    

    b) Run the code with a few different parameter values in order
       to estimate N, t_trial, and t_update for this code on
       a totient compute node.
       
    okay :) 

    c) Based on your model, suggest a strategy for choosing the batch
       size.  How might you generalize this strategy to automatically
       choose batch sizes for different types of computational
       experiments?
       
    Well it would basically be another load balancing problem of communication versus computation. 
    I personally think that having a maximal batch size and then removing communication
    between processors (say, with an array of size p storing results)
    and then doing a reduction with one processor after, would be much better
    
3.  In the workq subdirectory of this directory, there is a basic work
    queue implementation.  Following the strategy outlined in the
    slides, add synchronization calls in the locations marked TODO.
    You should run the code to make sure it behaves as expected!
    
    We discussed in class
